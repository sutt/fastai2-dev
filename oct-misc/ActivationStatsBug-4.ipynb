{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change none\n"
     ]
    }
   ],
   "source": [
    "def hook_fn(m, i, o):\n",
    "  print(f\"Working for layer: -- {m._get_name()} --\")\n",
    "\n",
    "awd_lstm_modified =  AWD_LSTM_M(vocab_sz=3,\n",
    "                  emb_sz=5,\n",
    "                  n_hid=6,\n",
    "                  n_layers=2)\n",
    "\n",
    "awd_lstm_original =  AWD_LSTM(vocab_sz=3,\n",
    "                  emb_sz=5,\n",
    "                  n_hid=6,\n",
    "                  n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(3, 5, padding_idx=1),\n",
       " Embedding(3, 5, padding_idx=1),\n",
       " LSTM(5, 6, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(6, 5, batch_first=True),\n",
       " ParameterModule(),\n",
       " RNNDropout(),\n",
       " RNNDropout(),\n",
       " RNNDropout()]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_model(awd_lstm_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(3, 5, padding_idx=1),\n",
       " LSTM(5, 6, batch_first=True),\n",
       " LSTM(6, 5, batch_first=True),\n",
       " RNNDropout(),\n",
       " RNNDropout(),\n",
       " RNNDropout()]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_model(awd_lstm_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AWD_LSTM_M(\n",
       "  (encoder): Embedding(3, 5, padding_idx=1)\n",
       "  (encoder_dp): Embedding(3, 5, padding_idx=1)\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(5, 6, batch_first=True)\n",
       "    (1): LSTM(6, 5, batch_first=True)\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AWD_LSTM(\n",
       "  (encoder): Embedding(3, 5, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(3, 5, padding_idx=1)\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(5, 6, batch_first=True)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(6, 5, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(3, 5, padding_idx=1),\n",
       " Embedding(3, 5, padding_idx=1),\n",
       " LSTM(5, 6, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(6, 5, batch_first=True),\n",
       " ParameterModule(),\n",
       " RNNDropout(),\n",
       " RNNDropout(),\n",
       " RNNDropout()]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_model(awd_lstm_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Embedding 1\n",
      "LSTM 2\n",
      "LSTM 3\n",
      "-------\n",
      "Embedding 1\n",
      "Embedding 2\n",
      "LSTM 3\n",
      "ParameterModule 4\n",
      "LSTM 5\n",
      "ParameterModule 6\n"
     ]
    }
   ],
   "source": [
    "for model in [awd_lstm_modified, awd_lstm_original]:\n",
    "    print('-------')\n",
    "    counter = 0\n",
    "    for m in flatten_model(model):\n",
    "        counter += 1\n",
    "        if has_params(m):\n",
    "            print(m._get_name(), str(counter))\n",
    "            m.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- LSTM --\n",
      "Working for layer: -- LSTM --\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0569,  0.1210,  0.0122,  0.0238, -0.0678],\n",
       "         [-0.0847,  0.1975,  0.0255,  0.0458, -0.0901],\n",
       "         [-0.0953,  0.2432,  0.0356,  0.0623, -0.0983],\n",
       "         [-0.0981,  0.2700,  0.0421,  0.0735, -0.1016]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_modified(torch.randint(3, (1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0673,  0.0205, -0.0193, -0.0155,  0.1494],\n",
       "         [ 0.1281,  0.0313, -0.0305, -0.0305,  0.2006],\n",
       "         [ 0.1716,  0.0351, -0.0360, -0.0394,  0.2196],\n",
       "         [ 0.2009,  0.0367, -0.0372, -0.0435,  0.2275]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_original(torch.randint(3, (1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the objects for _forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.models.awdlstm.AWD_LSTM_M"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(awd_lstm_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, <function __main__.hook_fn(m, i, o)>),\n",
       "             (10, <function __main__.hook_fn(m, i, o)>)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_modified.rnns[0]._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_original.rnns[0]._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(5, <function __main__.hook_fn(m, i, o)>),\n",
       "             (14, <function __main__.hook_fn(m, i, o)>)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_original.rnns[0].module._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(5, <function __main__.hook_fn(m, i, o)>),\n",
       "             (14, <function __main__.hook_fn(m, i, o)>)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_original.rnns[0].module._forward_hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is showing when the hooks get called\n",
    "When it WeightedDropout.forward() - no hooks called\n",
    "\n",
    "When WeightedDropout\\['module'].forward() is called - yes hooks called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- Embedding --\n"
     ]
    }
   ],
   "source": [
    "x_1 = torch.randint(3,(1,4))\n",
    "x_2 = awd_lstm_original.encoder(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightDropout(\n",
      "  (module): LSTM(5, 6, batch_first=True)\n",
      ")\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(awd_lstm_original.rnns[0])\n",
    "print('----')\n",
    "\n",
    "x_3 = awd_lstm_original.rnns[0](x_2) #.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(5, 6, batch_first=True)\n",
      "----\n",
      "Working for layer: -- LSTM --\n",
      "Working for layer: -- LSTM --\n"
     ]
    }
   ],
   "source": [
    "print(awd_lstm_original.rnns[0].module)\n",
    "print('----')\n",
    "\n",
    "x_3 = awd_lstm_original.rnns[0].module(x_2) #.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('devfastai': conda)",
   "language": "python",
   "name": "python37964bitdevfastaiconda0e0ad516d6eb4524903af3a3c31253d3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
